{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from data in Cloud Storage: data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def get_data_using_pandas(line_count):\n",
    "\n",
    "    # get data from aws s3\n",
    "    # url = \"s3://wagon-public-datasets/taxi-fare-train.csv\"\n",
    "    # df = pd.read_csv(url, nrows=100)\n",
    "\n",
    "    # load n lines from my csv\n",
    "    df = pd.read_csv(\"gs://le-wagon-data/data/train_1k.csv\", nrows=line_count)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data_using_blob(line_count):\n",
    "\n",
    "    # get data from aws s3\n",
    "    # url = \"s3://wagon-public-datasets/taxi-fare-train.csv\"\n",
    "\n",
    "    # get data from my google storage bucket\n",
    "    BUCKET_NAME = \"le-wagon-data\"\n",
    "    BUCKET_TRAIN_DATA_PATH = \"data/train_1k.csv\"\n",
    "\n",
    "    data_file = \"train_1k.csv\"\n",
    "\n",
    "    client = storage.Client()  # verifies $GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "    blob = bucket.blob(BUCKET_TRAIN_DATA_PATH)\n",
    "\n",
    "    blob.download_to_filename(data_file)\n",
    "\n",
    "    # load downloaded data to dataframe\n",
    "    df = pd.read_csv(data_file, nrows=line_count)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model to Cloud Storage: data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_gcp():\n",
    "\n",
    "    BUCKET_NAME = \"le-wagon-data\"\n",
    "    storage_location = \"models/random_forest_model.joblib\"\n",
    "    local_model_filename = \"model.joblib\"\n",
    "\n",
    "    client = storage.Client()\n",
    "\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "    blob = bucket.blob(storage_location)\n",
    "\n",
    "    blob.upload_from_filename(local_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train in the AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket\n",
    "BUCKET_NAME=le-wagon-data\n",
    "\n",
    "# training folder\n",
    "BUCKET_TRAINING_FOLDER=trainings\n",
    "\n",
    "# training params\n",
    "REGION=europe-west1\n",
    "\n",
    "# app environment\n",
    "PYTHON_VERSION=3.7\n",
    "FRAMEWORK=scikit-learn\n",
    "RUNTIME_VERSION=2.2\n",
    "\n",
    "# package params\n",
    "PACKAGE_NAME=taxifare\n",
    "FILENAME=trainer\n",
    "\n",
    "##### Job - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "JOB_NAME=taxi_fare_training_$(shell date +'%Y%m%d_%H%M%S')\n",
    "\n",
    "gcp_submit_training:\n",
    "\tgcloud ai-platform jobs submit training ${JOB_NAME} \\\n",
    "\t\t--job-dir gs://${BUCKET_NAME}/${BUCKET_TRAINING_FOLDER} \\\n",
    "\t\t--package-path ${PACKAGE_NAME} \\\n",
    "\t\t--module-name ${PACKAGE_NAME}.${FILENAME} \\\n",
    "\t\t--python-version=${PYTHON_VERSION} \\\n",
    "\t\t--runtime-version=${RUNTIME_VERSION} \\\n",
    "\t\t--region ${REGION} \\\n",
    "\t\t--stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANIFEST.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "include requirements.txt\n",
    "graft taxifare\n",
    "global-exclude *.py[cod] __pycache__ *.so *.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
