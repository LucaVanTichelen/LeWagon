
**The goal of today's recap is to go over the process of converting a training notebook into a training package.**

## Let's start by generating a new taxifare package

Create a new project `taxifare` in the projects directory.

<details>
  <summary markdown='span'><strong> ðŸ’¡ Hint </strong></summary>


ðŸ‘‰ Since `packgenlite` creates a git repository for us, we need to step outside of `data-challenges`: we want to avoid to create a git repository (the one generated by `packgenlite`) inside of another git repository (`data-challenges`)

```bash
cd ~/code/<user.github_nickname>
packgenlite taxifare
```

</details>
Since we want to be able to test and use our package in the **usage.ipynb** notebook as it is being built, we will install it using the `-e` flag...

``` bash
cd taxifare
pip install -e .
```

## Have a look at the training notebook

Let's open our package in order to edit our code:

```bash
code .
```

Let's open the **boilerplate.ipynb** and **usage.ipynb** notebooks and see how it works.

ðŸ‘‰ Open **another terminal window** for the notebooks. We will use the first terminal window in order to add and commit the code of our package as it evolves using `git` commands

``` bash
cd ~/code/<user.github_nickname>/data-challenges/07-Data-Engineering/02-ML-Iteration/Recap
jupyter notebook
```

ðŸ‘‰ `boilerplate.ipynb` is a model training notebook that contains the code that we want to refactor into a package.

ðŸ‘‰ `usage.ipynb` is the notebook that will be using our package once the code of `boilerplate.ipynb` has been refactored in our package.

Now that the jupyter server containing the notebooks with the code boilerplate is started, we will only use the first terminal window for our commands.

The goal of this recap is to progressively migrate and refactor the model training code, from the `boilerplate.ipynb` notebook to our package, while ensuring that our package works using the `usage.ipynb` notebook.

## Decompose the code into elements for our package

Now we will go through all the cells of `boilerplate.ipynb` one by one and progressively move the code into our package.

We may adopt the following structure for the code of the package:

``` bash
taxifare
â”œâ”€â”€ __init__.py
â”œâ”€â”€ data.py
â”œâ”€â”€ metrics.py
â”œâ”€â”€ mlflow.py
â”œâ”€â”€ model.py
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ trainer.py
â””â”€â”€ transformers
    â”œâ”€â”€ distance_transformer.py
    â””â”€â”€ utils.py
```

## Usage notebook

The goal is to refactor the code of the training notebook into the package so that the usage notebook can be run without errors.

<details>
  <summary markdown='span'><strong> ðŸ’¡ Hint </strong></summary>


You may progressively run the cells of the usage notebook in order to both see what the output of your package is. And in order to see the definitive output once the refactor is over.

</details>

## First step: Trainer class

The first step is to run without getting errors the **Trainer using our package** cell.

<details>
  <summary markdown='span'><strong> ðŸ’¡ Hint </strong></summary>


The package should be able to:
- Load some number of rows from the taxifare dataset
- Use a holdout
- Create a `Pipeline` containing the data cleaning and preprocessing
- Train a `RandomForestRegressor`
- Evaluate the performance
- Save the trained model to disk
- Save the parameters and metrics of the run to the [Le Wagon MLFlow server](https://mlflow.lewagon.co)

</details>


## Second step: ParamTrainer class

The second step is to run without getting errors the **Trainer with params and gridsearch** cell.

<details>
  <summary markdown='span'><strong> ðŸ’¡ Hint </strong></summary>


We want our package to be able to:
- Train on a combination of models and pipeline hyperparameters
- Use a `GridSearch` on our pipeline

</details>
