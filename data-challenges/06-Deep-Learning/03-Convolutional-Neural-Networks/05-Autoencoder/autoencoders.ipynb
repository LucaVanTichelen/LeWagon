{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "**Exercise objectives**\n",
    "- Discover autoencoders\n",
    "- Get a deeper understanding of CNNs\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "In this notebook, we look at a particular architecture used in deep learning: autoencoders. Autoencoders are neural network architectures trained to **output something as close as possible to the very input they were given**. It may seem strange but it's useful, we promise. \n",
    "\n",
    "The interest comes from the fact that there is a bottleneck in the network architecture i.e. a layer with a low number of neurons. If the autoencoder can reproduce its input, it means that the information that flows within the network is sufficient to recreate the input data. \n",
    "\n",
    "In particular, the **information contained at the bottleneck** - meaning the representation of the data at the low-dimensional layer - **accurately captures the data at hand and can recreate it**. It have many applications (compression, denoising etc...)\n",
    "\n",
    "<img src='https://github.com/lewagon/data-images/blob/master/DL/autoencoder.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data\n",
    "\n",
    "In this notebook, we will train an auto-encoder to work on 28x28 grey images from the MNIST dataset, available in keras. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "print(images_train.shape)\n",
    "print(images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a channels for the colors and normalize data\n",
    "X_train = images_train.reshape((60000, 28, 28, 1)) / 255.\n",
    "X_test = images_test.reshape((10000, 28, 28, 1)) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axs = plt.subplots(1, 10, figsize=(20, 4))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='Greys')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build the \"Encoder\" part for you (in blue in the network picture above)\n",
    "\n",
    "üí° Notice how it looks similar to a Convolution classifier of `latent_dimension` labels, except for the `tanh` activation of the final dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def build_encoder(latent_dimension):\n",
    "    '''returns an encoder model, of output_shape equals to latent_dimension'''\n",
    "    encoder = Sequential()\n",
    "    \n",
    "    encoder.add(Conv2D(8, (2,2), input_shape=(28, 28, 1), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "\n",
    "    encoder.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))     \n",
    "\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(latent_dimension, activation='tanh'))\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Build your encoder with  `latent_dimension=2` and look at the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn to build the decoder this time!\n",
    "\n",
    "We need to build a reverse CNN that takes a dense layer as input, and output image of shape `(28,28,1)` similar to our MNIST images. \n",
    "\n",
    "For that, we will use a new layer called [`Conv2DTranspose`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose), which does what it say: the opposite of a convolution\n",
    "\n",
    "üí° We could start by reshaping the Dense input layer into images of shape `(7,7,..)`, then apply to `Conv2DTranspose` with `stride=2` to double its image shape to `(14,14,..)` then another one up to `(28,28,1)`\n",
    "\n",
    "‚ùì **Question** ‚ùì Define a the decoder architecture in the method below as follow:\n",
    "- a `Dense` layer with `7*7*8` neurons, and input shape `(latent_dimension,)` and the `tanh` activation function. \n",
    "- a Reshape layer that reshapes to `(7, 7, 8)` tensors\n",
    "- a Conv2DTranspose with `8` filters, `(2,2)` kernels, strides of `2`, padding `same` and activation being `relu`\n",
    "- a second Conv2DTranspose layer with `1` filter, `(2,2)` kernels, strides of `2`, padding `same`, and the `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "\n",
    "def build_decoder(latent_dimension):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Build your decoder with `latent_dimension=2` and check that it outputs images of same shape than the encoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now concatenate both the encoder and the decoder thanks to the `Model` class in Keras, using the `functionalAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    inp = Input((28, 28,1))\n",
    "    encoded = encoder(inp)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(inp, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Try to understand syntax above, build your autoencoder and look at the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Defines a method which compiles your model. Pick an appropriate loss.\n",
    "\n",
    "Think carefully: on which mathematical objects are we going to compare predictions and ground truth for the computation of loss and the metric?\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>üÜò Answer</summary>\n",
    "\n",
    "It should compare two images (Black and White in our case), pixel-by-pixel!\n",
    "    \n",
    "The MSE loss seems appropriate for pixel-by-pixel error minimization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì  Compile your model and fit it with  `batch_size = 32` and `epochs=20`. What is the label `y` in this case?\n",
    "\n",
    "**Note:** In this notebook, always set. The goal of this exercise is not to carefully deal with overfitting or to perfectly train the models but to understand autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at predicted images from the autoencoder, are they close to the original ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Using only the encoder part of the network, encode your dataset and save it under `X_encoded` . \n",
    "\n",
    "Each image is now represented by two values (that correspond to the dimension of the latent space, of the bottleneck; aka the `latent_dimension`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Each encoded 2D-datapoint corresponds to a given label, between 0 and 9 (which is the initial written data). \n",
    "\n",
    "Represent on a 2D plot the encoded data (only a small subset of it for visibility purpose\n",
    "- Each point of the scatter plot will correspond to an encoded image\n",
    "- Color the dot according to the label (digit representation) it corresponds to.\n",
    "- For instance, all the \"4\" should be represented by a color on this scatter plot, while the \"5\" should be represented by another color.\n",
    "\n",
    "What do you remark on this plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Application: Image denoising\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì We will here add some noise to the input data. Run the following code and plot pair of initial and related noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise_factor = 0.5\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(0., 1., size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(0., 1., size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, reinitialize your autoencoder (with a latent space of 2) and train it to predict the denoised image from the noisy one. \n",
    "\n",
    "(keep batch_size = 32 and epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì For some noisy test data, predict the denoised data and plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, try to evaluate which `latent_dimension` is the best in order to have the best image reconstruction (aka denoise the data as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}