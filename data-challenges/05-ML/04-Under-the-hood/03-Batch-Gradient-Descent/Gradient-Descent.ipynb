{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import for you the following dataset of ingredients with their mineral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliment</th>\n",
       "      <th>zinc</th>\n",
       "      <th>phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Durum wheat pre-cooked. whole grain. cooked. u...</td>\n",
       "      <td>0.120907</td>\n",
       "      <td>0.193784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian noodles. plain. cooked. unsalted</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.060329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rice. brown. cooked. unsalted</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.201097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice. cooked. unsalted</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rice. parboiled. cooked. unsalted</td>\n",
       "      <td>0.025189</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliment      zinc  phosphorus\n",
       "0  Durum wheat pre-cooked. whole grain. cooked. u...  0.120907    0.193784\n",
       "1             Asian noodles. plain. cooked. unsalted  0.047859    0.060329\n",
       "2                      Rice. brown. cooked. unsalted  0.156171    0.201097\n",
       "3                             Rice. cooked. unsalted  0.065491    0.045704\n",
       "4                  Rice. parboiled. cooked. unsalted  0.025189    0.045704"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_ingredients_zinc_phosphorous.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We can visualize a somewhat Linear relationship between the `Phosphorus` and `Zinc`.   \n",
    "\n",
    "Let's use Gradient Descent to find the line of best fit between them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=data, x='zinc', y='phosphorus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Create the two `np.Array`\n",
    "- `data_X` for zinc\n",
    "- `data_Y` for phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_X = np.array(data.zinc)\n",
    "data_Y = np.array(data.phosphorus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_X.shape == (53,))\n",
    "assert (data_Y.shape == (53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercise, you will define the key functions used to update the parameters during one epoch $\\color {red}{(k)}$ of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "üëá Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,a,b):\n",
    "    return a * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "üëá Define the SSR Loss Function for the above created Hypothesis Function. Reuse `h` coded above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(X,Y,a,b):\n",
    "    return np.sum((Y - h(X,a,b)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the total Loss computed on all our ingredients dataset if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.868506986115456"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "loss(data_X, data_Y, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2  x_i (y^{(i)} - \\hat{y}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "üëá Define a function to compute the partial derivatives of the Loss Function relative to parameter `a` and `b` at a given points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Again, you must use the Hypothesis Function within to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,a,b):\n",
    "    d_a = np.sum(-2 * data_X * (Y - h(X,a,b)))\n",
    "    d_b = np.sum(-2 * (Y - h(X,a,b)))\n",
    "    return d_a, d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "d_a, d_b = gradient(data_X, data_Y, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Define a function that calculates the step sizes alongside each parameter (`a`,`b`), according to their derivatives (`d_a`, `d_b`) and a learning_rate equals to 0.01 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(d_a,d_b, learning_rate = 0.01):\n",
    "    step_a = d_a * learning_rate\n",
    "    step_b = d_b * learning_rate\n",
    "    return (step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the steps (`step_a`, `step_b`) to take for the derivatives computed above for (`a`,`b`) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "step_a, step_b = steps(d_a, d_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The steps should be 0.48 for a and 1.15 for b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update parameters (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(a, b, step_a, step_b):\n",
    "    a_new = a - step_a\n",
    "    b_new = b - step_b\n",
    "    return a_new , b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5154093419089099, -0.1517923733301405)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_params(a, b , step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the value of the loss, a, and b at each epoch to a list called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "a_history = []\n",
    "b_history = []\n",
    "a = 1\n",
    "b = 1\n",
    "for i in range(100):\n",
    "    a_history.append(a)\n",
    "    b_history.append(b)\n",
    "    loss_history.append(loss(data_X, data_Y, a, b))\n",
    "    d_a, d_b = gradient(data_X, data_Y, a, b)\n",
    "    step_a, step_b = steps(d_a, d_b)\n",
    "    a, b = update_params(a, b, step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the parameter values `a_100` and `b_100` at the end of the 100 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "a_100 = a_history[99]\n",
    "b_100 = b_history[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7686596499806505"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /home/useradd/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/useradd/code/LucaVanTichelen/data-challenges/05-ML/04-Under-the-hood/03-Batch-Gradient-Descent\n",
      "plugins: dash-2.0.0, anyio-3.3.2\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_descent.py::TestDescent::test_a \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 50%]\u001b[0m\n",
      "tests/test_descent.py::TestDescent::test_b \u001b[32mPASSED\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/descent.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed descent step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "# üß™ Test your code\n",
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('descent',\n",
    "                         a_100=a_100,\n",
    "                         b_100=b_100)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Wrap this iterative approach into a method `gradient_descent()` which returns your new a/b and `history`, a dictionary containing the \n",
    "- `loss_history`\n",
    "- `a_history`\n",
    "- `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    pass  # YOUR CODE HERE\n",
    "    return a_new, b_new, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize your descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to plot our loss function and the descent steps on a 2D surface using matplotlib [contourf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëáStart by creating the data we need for the plot\n",
    "- `range_a` a range of 100 values for `a` equally spaced between -1 and 1\n",
    "- `range_b` a range of 100 values for `b` equally spaced between -1 and 1 \n",
    "- `Z` a 2D-array where each elements `Z[j,i]` is equal to the value of the loss function at `a` = `range_a[i]` and `b` = `range_b[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Now, plot in one single subplot:\n",
    "- your gradient as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html)\n",
    "- all historical (a,b) points as red dots to visualize your gradient descent!\n",
    "\n",
    "Change your learning rate and observe it's impact on the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá [optional] What about 3D? Try out this [plot.ly - 3D contour plot](https://plotly.com/python/3d-surface-plots/) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "surface = go.Surface(x=range_a, y=range_b, z=Z)\n",
    "scatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\n",
    "fig = go.Figure(data=[surface, scatter])\n",
    "\n",
    "#fig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot the history of the `loss` values as a function of number of `epochs`. Vary the `learning_rate` from 0.001 to 0.01 and make sure to understand the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†üèÅ Congratulation! Please, push your exercise when you are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
